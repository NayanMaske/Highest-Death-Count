//importing package
package org.myorg;
// importing libraries than contains the required classes and methods 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.fs.FileSystem;

public class MaxDeaths
{

	public static void main(String[] args) throws Exception// entry point to the application
	{
		//create the “conf” object from the “Configuration” class, which provides access to
		 //configuration parameters necessary for Hadoop job

		Configuration conf = new Configuration();
		if (args.length != 3)
		{
			System.err.println("Usage: MaxDeaths <input path> <output path>");
			System.exit(-1);
		}

		Job job;
		job=Job.getInstance(conf, "Max Deaths");
		//specifying the driver class in the JAR file
		job.setJarByClass(MaxDeaths.class);

		FileInputFormat.addInputPath(job, new Path(args[1]));
		FileOutputFormat.setOutputPath(job, new Path(args[2]));

		job.setMapperClass(MaxDeathsMapper.class);
		job.setReducerClass(MaxDeathsReducer.class);
		//Setting the mapper and reducer for the job
		//job.setCombinerClass(MaxDeathsReducer.class);

		
		//Setting the data type for the output key class to be Hadoop’s data type Text and
		 //Setting the data type for the output Value class to be Hadoop’s data type DoubleWritable
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(DoubleWritable.class);

      		// Delete output if exists
	   	FileSystem hdfs = FileSystem.get(conf);
        		Path outputDir = new Path(args[2]);
		if (hdfs.exists(outputDir))
          	    hdfs.delete(outputDir, true);

		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}

